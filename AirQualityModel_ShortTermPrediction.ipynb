{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f94d432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b61875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "def ann_main(df, predays, hl_count, hl_nodes, batch_size, epochs):\n",
    "\n",
    "    test_df = df[['temp','humidity','precip','windspeed','Season']].copy()\n",
    "    \n",
    "    encoder = ce.OneHotEncoder(cols='Season',handle_unknown='return_nan', return_df = True, use_cat_names=True)\n",
    "    df = encoder.fit_transform(df)\n",
    "    \n",
    "    for x in range(1,predays+1):\n",
    "        test_df=test_df.shift(periods=1,fill_value=0)\n",
    "        df = df.join(test_df[['temp','humidity','precip','windspeed','Season']],rsuffix=\"_pr\"+str(x))\n",
    "\n",
    "    X = df.iloc[:, [df.columns.get_loc('temp'),df.columns.get_loc('humidity'), \n",
    "                    df.columns.get_loc('precip'), df.columns.get_loc('windspeed')]].values\n",
    "\n",
    "    for x in range(1,predays+1):\n",
    "        Xnew = df.iloc[:, [df.columns.get_loc('temp_pr'+str(x)),df.columns.get_loc('humidity_pr'+str(x)), \n",
    "                    df.columns.get_loc('precip_pr'+str(x)), df.columns.get_loc('windspeed_pr'+str(x))]].values\n",
    "        X = np.concatenate((X,Xnew),axis=1)\n",
    "    \n",
    "    y = df.iloc[:, df.columns.get_loc('aqi')].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    x_row, x_col = X.shape\n",
    "    ann_input_size = x_col\n",
    "\n",
    "    ann = keras.Sequential()\n",
    "    ann.add(Dense(hl_nodes, activation = 'relu', input_dim = ann_input_size))\n",
    "    for hidden_layer in range(hl_count):\n",
    "        ann.add(Dense(hl_nodes, activation = 'relu'))\n",
    "    ann.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "    ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    ann.fit(X_train, y_train, batch_size, epochs)\n",
    "\n",
    "    tr_pred = ann.predict(X_train)\n",
    "    tr_scores = ann.evaluate(X_train, y_train, verbose = 0)\n",
    "    print(\"Acc on train data: {}% \\n error on train: {}\".format(tr_scores[1], (1-tr_scores[1])))\n",
    "\n",
    "    te_scores = ann.evaluate(X_test, y_test, verbose = 0)\n",
    "    print(\"Acc on test data: {}% \\n error on test: {}\".format(te_scores[1], 1-te_scores[1]))\n",
    "\n",
    "    te_pred = ann.predict(X_test)\n",
    "    te_pred = (te_pred > 0.5).astype(int)\n",
    "    cm = confusion_matrix(y_true=y_test, y_pred=te_pred)\n",
    "\n",
    "    cm_plot_labels = ['good_aqi','bad_aqi']\n",
    "    #plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')\n",
    "\n",
    "    f1score = f1_score(y_true=y_test, y_pred=te_pred)\n",
    "    print(\"f1Score: {}\".format(f1score))\n",
    "    \n",
    "    return (te_scores[1], f1score)\n",
    "\n",
    "   \n",
    "    \n",
    "def ann_run(filename, prevdays, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max):\n",
    "\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    df[\"aqi\"] = (df['aqi'] >= 25.0).astype(int)\n",
    "\n",
    "    df_res = pd.DataFrame(columns=['previous_days','hl_count', 'hl_nodes','batch_size', 'epochs', 'f1_score', 'accuracy'])\n",
    "\n",
    "    for predays in range(prevdays+1):\n",
    "        for hl_count in range(1,hl_count_max+1):\n",
    "            for hl_nodes in range(hl_nodes_min, hl_nodes_max+1):\n",
    "                for batch in range(batch_min,batch_max+1):\n",
    "                    for ep in range(ep_min,ep_max+1):\n",
    "                        f1Score, acc = ann_main(df = df, predays = predays, hl_count = hl_count, hl_nodes = hl_nodes,batch_size = batch, epochs = ep)\n",
    "                        new_row = {'previous_days':predays, 'hl_count':hl_count, 'hl_nodes':hl_nodes, 'batch_size':batch, 'epochs':ep, 'f1_score':f1Score, 'accuracy':acc}\n",
    "                        df_res = df_res.append(new_row, ignore_index=True)       \n",
    "                        \n",
    "    print(\"ANN run done\")\n",
    "    \n",
    "    return (df_res)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a51a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevdays = 5\n",
    "hl_count_max = 5\n",
    "hl_nodes_min = 8\n",
    "hl_nodes_max= 12\n",
    "batch_min= 10\n",
    "batch_max=10\n",
    "ep_min=10 \n",
    "ep_max=10\n",
    "df_res = ann_run(\"../Data_Prep_LA/WeatherAndPollution_2011_2020.csv\", prevdays, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max)\n",
    "print(df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2473d7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevdays = 5\n",
    "hl_count_max = 5\n",
    "hl_nodes_min = 8\n",
    "hl_nodes_max= 12\n",
    "batch_min= 10\n",
    "batch_max=10\n",
    "ep_min=10 \n",
    "ep_max=10\n",
    "df_res = ann_run(\"../Data_Prep_NY/WeatherAndPollution_2011_2020.csv\", prevdays, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max)\n",
    "df_res.to_csv(\"NY_Results_20230129.csv\",index=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42f642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevdays = 5\n",
    "hl_count_max = 5\n",
    "hl_nodes_min = 8\n",
    "hl_nodes_max= 12\n",
    "batch_min= 10\n",
    "batch_max=10\n",
    "ep_min=10 \n",
    "ep_max=10\n",
    "df_res = ann_run(\"../Data_Prep_FL/WeatherAndPollution_2011_2020.csv\", prevdays, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max)\n",
    "df_res.to_csv(\"FL_Results_20230129.csv\",index=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7bbebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevdays = 5\n",
    "hl_count_max = 5\n",
    "hl_nodes_min = 8\n",
    "hl_nodes_max= 12\n",
    "batch_min= 10\n",
    "batch_max=10\n",
    "ep_min=10 \n",
    "ep_max=10\n",
    "df_res = ann_run(\"../Data_Prep_SF/WeatherAndPollution_2011_2020.csv\", prevdays, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max)\n",
    "df_res.to_csv(\"SF_Results_20230129.csv\",index=False)\n",
    "print(\"done\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

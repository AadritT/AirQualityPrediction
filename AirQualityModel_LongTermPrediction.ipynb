{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f94d432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b61875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "def ann_main(df, predays, hl_count, hl_nodes, batch_size, epochs, futdays, season_flag):\n",
    "\n",
    "    \n",
    "    if (season_flag):\n",
    "        encoder = ce.OneHotEncoder(cols='Season',handle_unknown='return_nan', return_df = True, use_cat_names=True)\n",
    "        df = encoder.fit_transform(df)\n",
    "    else:\n",
    "        df = df.drop(columns = ['Season'])\n",
    "    \n",
    "    labels = [\"good\", \"moderate\", \"unhealthy-sensitive\", \"unhealthy\", \"very-unhealthy\", 'hazardous']\n",
    "    bins = [0, 50, 100, 150, 200, 300, 500]\n",
    "    df['aqi_categories'] = pd.cut(df['aqi'], bins, labels=labels, include_lowest=True)\n",
    "    \n",
    "    yle = LabelEncoder()\n",
    "    df['aqi'] = yle.fit_transform(df['aqi_categories'])\n",
    "    \n",
    "    #shifts the 3 things to the front to make adding prevs easier\n",
    "    df.insert(0, \"omean\", df.pop(\"omean\"))\n",
    "    df.insert(0, \"aqi\", df.pop(\"aqi\"))\n",
    "    df.insert(0, \"pmean\", df.pop(\"pmean\"))\n",
    "    df.insert(0, \"aqi_categories\", df.pop(\"aqi_categories\"))\n",
    "    \n",
    "    ax = df['aqi'].plot.hist()\n",
    "    \n",
    "    if (season_flag):\n",
    "        test_df = df[['temp','humidity','precip','windspeed', \"Season_0.0\",\"Season_1.0\",\"Season_2.0\",\"Season_3.0\"]].copy()\n",
    "        for x in range(1,predays+1):\n",
    "            test_df=test_df.shift(periods=1,fill_value=0)\n",
    "            df = df.join(test_df[['temp','humidity','precip','windspeed',\"Season_0.0\",\"Season_1.0\",\"Season_2.0\",\"Season_3.0\"]],rsuffix=\"_pr\"+str(x))\n",
    "    else:\n",
    "        test_df = df[['temp','humidity','precip','windspeed']].copy()\n",
    "        for x in range(1,predays+1):\n",
    "            test_df=test_df.shift(periods=1,fill_value=0)\n",
    "            df = df.join(test_df[['temp','humidity','precip','windspeed']],rsuffix=\"_pr\"+str(x))\n",
    "                 \n",
    "   \n",
    "#     X = df.iloc[:, [df.columns.get_loc('temp'),df.columns.get_loc('humidity'), \n",
    "#                     df.columns.get_loc('precip'), df.columns.get_loc('windspeed'), ]].values\n",
    "    \n",
    "    #print(df.head(5))\n",
    "    X = df.iloc[:, 5:].values\n",
    "    \n",
    "    \n",
    "    df['aqi'] = df.aqi.shift(-1*futdays)\n",
    "    df = df.fillna(method='ffill')\n",
    "    y = df.iloc[:, df.columns.get_loc('aqi')].values\n",
    "    y = to_categorical(y, num_classes=6)\n",
    "    print(y.shape)\n",
    "    print(y)\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    x_row, x_col = X.shape\n",
    "    ann_input_size = x_col\n",
    "\n",
    "    ann = keras.Sequential()\n",
    "    ann.add(Dense(hl_nodes, activation = 'relu', input_dim = ann_input_size))\n",
    "    for hidden_layer in range(hl_count):\n",
    "        ann.add(Dense(hl_nodes, activation = 'relu'))\n",
    "    ann.add(Dense(6, activation = 'softmax'))\n",
    "\n",
    "    ann.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    ann.fit(X_train, y_train, batch_size, epochs)\n",
    "\n",
    "    tr_pred = ann.predict(X_train)\n",
    "    tr_scores = ann.evaluate(X_train, y_train, verbose = 0)\n",
    "    print(\"Acc on train data: {}% \\n error on train: {}\".format(tr_scores[1], (1-tr_scores[1])))\n",
    "\n",
    "    te_scores = ann.evaluate(X_test, y_test, verbose = 0)\n",
    "    print(\"Acc on test data: {}% \\n error on test: {}\".format(te_scores[1], 1-te_scores[1]))\n",
    "\n",
    "    te_pred = ann.predict(X_test)\n",
    "    te_pred = (te_pred > 0.5).astype(int)\n",
    "    #cm = confusion_matrix(y_true=y_test, y_pred=te_pred)\n",
    "\n",
    "    #cm_plot_labels = ['good_aqi','bad_aqi']\n",
    "    #plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')\n",
    "\n",
    "    f1score = f1_score(y_true=y_test, y_pred=te_pred, average='weighted')\n",
    "    print(\"f1Score: {}\".format(f1score))\n",
    "    \n",
    "    return (te_scores[1], tr_scores[1], f1score)\n",
    "\n",
    "   \n",
    "    \n",
    "def ann_run(filename, prevday_max, prevday_min, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, futdays,s_flag):\n",
    "\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    df_res = pd.DataFrame(columns=['previous_days','hl_count', 'hl_nodes','batch_size', 'epochs', 'future_days', 'f1_score', 'testing_accuracy','training_accuracy'])\n",
    "\n",
    "    for futday in range(futdays+1):\n",
    "        for predays in range(prevday_min, prevday_max+1):\n",
    "            for hl_count in range(hl_count_min,hl_count_max+1):\n",
    "                for hl_nodes in range(hl_nodes_min, hl_nodes_max+1):\n",
    "                    for batch in range(batch_min,batch_max+1):\n",
    "                        for ep in range(ep_min,ep_max+1, ep_step):\n",
    "                            te_acc, tr_acc, f1Score = ann_main(df = df, predays = predays, hl_count = hl_count, hl_nodes = hl_nodes,batch_size = batch, epochs = ep, futdays = futday, season_flag=s_flag)\n",
    "                            new_row = {'previous_days':predays, 'hl_count':hl_count, 'hl_nodes':hl_nodes, 'batch_size':batch, 'epochs':ep, 'future_days':futday, 'f1_score':f1Score, 'testing_accuracy':te_acc, 'training_accuracy':tr_acc}\n",
    "                            df_res = df_res.append(new_row, ignore_index=True)       \n",
    "                        \n",
    "    print(\"ANN run done\")\n",
    "    \n",
    "    return (df_res)\n",
    "    \n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2473d7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevday_min = 1\n",
    "prevday_max = 1\n",
    "hl_count_max = 1\n",
    "hl_nodes_min = 10\n",
    "hl_nodes_max= 10\n",
    "batch_min= 10\n",
    "batch_max=10\n",
    "ep_min=10 \n",
    "ep_max=10\n",
    "ep_step = 5\n",
    "futdays = 0\n",
    "df_res = ann_run(\"../Data_Prep_LA/WeatherAndPollution_2011_2020.csv\", prevday_max, prevday_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, futdays, s_flag = False)\n",
    "df_res.to_csv(\"LA_Results_20230212.csv\",index=False)\n",
    "print(df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42f642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevday_min = 1\n",
    "prevday_max = 1\n",
    "hl_count_min = 5\n",
    "hl_count_max = 5\n",
    "hl_nodes_min = 10\n",
    "hl_nodes_max= 10\n",
    "batch_min= 10\n",
    "batch_max=10\n",
    "ep_min=10 \n",
    "ep_max=10\n",
    "ep_step = 5\n",
    "futdays = 0\n",
    "df_res = ann_run(\"../Data_Prep_SF/WeatherAndPollution_2011_2020.csv\", prevday_max, prevday_min, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, futdays, s_flag = False)\n",
    "df_res.to_csv(\"SF_Results_20230212.csv\",index=False)\n",
    "print(df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7bbebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevday_min = 0\n",
    "prevday_max = 0\n",
    "hl_count_max = 1\n",
    "hl_nodes_min = 10\n",
    "hl_nodes_max= 10\n",
    "batch_min= 10\n",
    "batch_max=10\n",
    "ep_min=10 \n",
    "ep_max=10\n",
    "futdays = 0\n",
    "df_res = ann_run(\"../Data_Prep_FL/WeatherAndPollution_2011_2020.csv\", prevday_max, prevday_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max,futdays)\n",
    "df_res.to_csv(\"FL_Results_20230212.csv\",index=False)\n",
    "print(df_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce99fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevday_min = 0\n",
    "prevday_max = 0\n",
    "hl_count_max = 1\n",
    "hl_nodes_min = 10\n",
    "hl_nodes_max= 10\n",
    "batch_min= 10\n",
    "batch_max=10\n",
    "ep_min=10 \n",
    "ep_max=10\n",
    "futdays = 0\n",
    "df_res = ann_run(\"../Data_Prep_NY/WeatherAndPollution_2011_2020.csv\", prevday_max, prevday_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max,futdays)\n",
    "df_res.to_csv(\"NY_Results_20230212.csv\",index=False)\n",
    "print(df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2a4910",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevday_min = 0\n",
    "prevday_max = 0\n",
    "hl_count_max = 1\n",
    "hl_nodes_min = 10\n",
    "hl_nodes_max= 10\n",
    "batch_min= 10\n",
    "batch_max=10\n",
    "ep_min=10 \n",
    "ep_max=10\n",
    "ep_step = 1\n",
    "futdays = 0\n",
    "df_res = ann_run(\"../Data_Prep_ALL/WeatherAndPollution_2011_2020.csv\", prevday_max, prevday_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, futdays)\n",
    "df_res.to_csv(\"ALL_Results_20230212.csv\",index=False)\n",
    "print(df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354f180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filename = \"../ANN/LA_Results_20230129.csv\" #up to PyCode, down to ANN\n",
    "df_results = pd.read_csv(filename)\n",
    "df_results = df_results.drop(df_results.columns[0], axis=1)\n",
    "\n",
    "for prev in range(0,6):\n",
    "    label = \"N\"+str(prev)+\"C\"+str(hl_nodes)\n",
    "    specific_df = df_results.loc[(df_results['previous_days'] == prev) & (df_results['hl_nodes']==hl_nodes)]\n",
    "    plt.plot(specific_df['hl_count'], specific_df['accuracy'], label=label) #plots\n",
    "\n",
    "#plt.legend(loc=\"best\")\n",
    "plt.ylim(0.75,0.85)\n",
    "\n",
    "#plt.show() #displays plot\n",
    "plt.savefig(fname=\"New_York_HL_Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecada6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runHypTun(infile, pre):\n",
    "    prevday_min = 1\n",
    "    prevday_max = 1\n",
    "    hl_count_min = 5\n",
    "    hl_count_max = 5\n",
    "    hl_nodes_min = 10\n",
    "    hl_nodes_max= 10\n",
    "    batch_min= 10\n",
    "    batch_max=10\n",
    "    ep_min=10 \n",
    "    ep_max=10\n",
    "    ep_step = 1\n",
    "    futdays = 0\n",
    "\n",
    "    hl_nodes_min = 5\n",
    "    hl_nodes_max= 15\n",
    "    df_res = ann_run(infile, prevday_max, prevday_min, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, futdays, s_flag = False)\n",
    "    df_res.to_csv(\"../HypTun/\"+pre+\"_Results_HLNCount.csv\",index=False)\n",
    "    hl_nodes_min = 10\n",
    "    hl_nodes_max= 10\n",
    "\n",
    "\n",
    "    hl_count_min = 2\n",
    "    hl_count_max= 10\n",
    "    df_res = ann_run(infile, prevday_max, prevday_min, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, futdays, s_flag = False)\n",
    "    df_res.to_csv(\"../HypTun/\"+pre+\"_Results_HLCount.csv\",index=False)\n",
    "    hl_count_min = 5\n",
    "    hl_count_max = 5\n",
    "\n",
    "    ep_min = 5\n",
    "    ep_max = 50\n",
    "    ep_step = 5\n",
    "    df_res = ann_run(infile, prevday_max, prevday_min, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, futdays, s_flag = False)\n",
    "    df_res.to_csv(\"../HypTun/\"+pre+\"_Results_Epochs.csv\",index=False)\n",
    "    ep_min=10 \n",
    "    ep_max=10\n",
    "    ep_step = 1\n",
    "    \n",
    "\n",
    "    batch_min = 5 \n",
    "    batch_max= 15\n",
    "    df_res = ann_run(infile, prevday_max, prevday_min, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, futdays, s_flag = False)\n",
    "    df_res.to_csv(\"../HypTun/\"+pre+\"_Results_Batch.csv\",index=False)\n",
    "    batch_min= 10\n",
    "    batch_max=10\n",
    "\n",
    "\n",
    "    prevday_min = 0\n",
    "    prevday_max= 5\n",
    "    df_res = ann_run(infile, prevday_max, prevday_min, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, futdays, s_flag = False)\n",
    "    df_res.to_csv(\"../HypTun/\"+pre+\"_Results_Predays.csv\",index=False)\n",
    "    prevday_min = 1\n",
    "    prevday_max = 1\n",
    "\n",
    "\n",
    "    futdays = 14\n",
    "    df_res = ann_run(infile, prevday_max, prevday_min, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, futdays, s_flag = False)\n",
    "    df_res.to_csv(\"../HypTun/\"+pre+\"_Results_Futdays.csv\",index=False)\n",
    "    futdays=0\n",
    "\n",
    "runHypTun(\"../Data_Prep_LA/WeatherAndPollution_2011_2020.csv\", \"LA\")\n",
    "runHypTun(\"../Data_Prep_SF/WeatherAndPollution_2011_2020.csv\", \"SF\")\n",
    "runHypTun(\"../Data_Prep_FL/WeatherAndPollution_2011_2020.csv\", \"FL\")\n",
    "runHypTun(\"../Data_Prep_NY/WeatherAndPollution_2011_2020.csv\", \"NY\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7abb84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevday_min = 0\n",
    "prevday_max = 5\n",
    "hl_count_min = 2\n",
    "hl_count_max = 10\n",
    "hl_nodes_min = 5\n",
    "hl_nodes_max= 15\n",
    "batch_min= 5\n",
    "batch_max=15\n",
    "ep_min=5 \n",
    "ep_max=50\n",
    "ep_step = 5\n",
    "futdays = 14\n",
    "df_res = ann_run(\"../Data_Prep_Baker/WeatherAndPollution_2011_2020.csv\", prevday_max, prevday_min, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, futdays, s_flag = False)\n",
    "df_res.to_csv(\"Baker_Results_20230212.csv\",index=False)\n",
    "print(df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a14c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runHypTun(pre):\n",
    "    filename = \"../Data_Prep_Baker/WeatherAndPollution_2011_2020.csv\"\n",
    "    prevday_min = 1\n",
    "    prevday_max = 1\n",
    "    hl_count_min = 5\n",
    "    hl_count_max = 5\n",
    "    hl_nodes_min = 10\n",
    "    hl_nodes_max= 10\n",
    "    batch_min= 10\n",
    "    batch_max=10\n",
    "    ep_min=10 \n",
    "    ep_max=10\n",
    "    ep_step = 1\n",
    "    futdays = 0\n",
    "\n",
    "    hl_nodes_min = 5\n",
    "    hl_nodes_max= 15\n",
    "    df_res = ann_run(filename, prevday_max, prevday_min, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, futdays, s_flag = False)\n",
    "    df_res.to_csv(\"../HypTun/\"+pre+\"_Results_HLNCount.csv\",index=False)\n",
    "    hl_nodes_min = 10\n",
    "    hl_nodes_max= 10\n",
    "\n",
    "\n",
    "    hl_count_min = 2\n",
    "    hl_count_max= 10\n",
    "    df_res = ann_run(filename, prevday_max, prevday_min, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, futdays, s_flag = False)\n",
    "    df_res.to_csv(\"../HypTun/\"+pre+\"_Results_HLCount.csv\",index=False)\n",
    "    hl_count_min = 5\n",
    "    hl_count_max = 5\n",
    "\n",
    "    ep_min = 5\n",
    "    ep_max = 50\n",
    "    ep_step = 5\n",
    "    df_res = ann_run(filename, prevday_max, prevday_min, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, futdays, s_flag = False)\n",
    "    df_res.to_csv(\"../HypTun/\"+pre+\"_Results_Epochs.csv\",index=False)\n",
    "    ep_min=10 \n",
    "    ep_max=10\n",
    "    ep_step = 1\n",
    "    \n",
    "\n",
    "    batch_min = 5 \n",
    "    batch_max= 15\n",
    "    df_res = ann_run(filename, prevday_max, prevday_min, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, futdays, s_flag = False)\n",
    "    df_res.to_csv(\"../HypTun/\"+pre+\"_Results_Batch.csv\",index=False)\n",
    "    batch_min= 10\n",
    "    batch_max=10\n",
    "\n",
    "\n",
    "    prevday_min = 0\n",
    "    prevday_max= 5\n",
    "    df_res = ann_run(filename, prevday_max, prevday_min, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, futdays, s_flag = False)\n",
    "    df_res.to_csv(\"../HypTun/\"+pre+\"_Results_Predays.csv\",index=False)\n",
    "    prevday_min = 1\n",
    "    prevday_max = 1\n",
    "\n",
    "\n",
    "    futdays = 14\n",
    "    df_res = ann_run(filename, prevday_max, prevday_min, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, futdays, s_flag = False)\n",
    "    df_res.to_csv(\"../HypTun/\"+pre+\"_Results_Futdays.csv\",index=False)\n",
    "    futdays=0\n",
    "\n",
    "runHypTun(\"Baker\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0049e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runHypTun(pre):\n",
    "    filename = \"../Data_Prep_\"+pre+\"/WeatherAndPollution_2011_2020.csv\"\n",
    "    prevday_min = 1\n",
    "    prevday_max = 1\n",
    "    hl_count_min = 5\n",
    "    hl_count_max = 5\n",
    "    hl_nodes_min = 10\n",
    "    hl_nodes_max= 10\n",
    "    batch_min= 10\n",
    "    batch_max=10\n",
    "    ep_min=10 \n",
    "    ep_max=10\n",
    "    ep_step = 1\n",
    "    futdays = 0\n",
    "\n",
    "    hl_nodes_min = 5\n",
    "    hl_nodes_max= 15\n",
    "    df_res = ann_run(filename, prevday_max, prevday_min, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, futdays, s_flag = False)\n",
    "    df_res.to_csv(\"../HypTun/\"+pre+\"_Results_HLNCount.csv\",index=False)\n",
    "    hl_nodes_min = 10\n",
    "    hl_nodes_max= 10\n",
    "\n",
    "\n",
    "    hl_count_min = 2\n",
    "    hl_count_max= 10\n",
    "    df_res = ann_run(filename, prevday_max, prevday_min, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, futdays, s_flag = False)\n",
    "    df_res.to_csv(\"../HypTun/\"+pre+\"_Results_HLCount.csv\",index=False)\n",
    "    hl_count_min = 5\n",
    "    hl_count_max = 5\n",
    "\n",
    "    ep_min = 5\n",
    "    ep_max = 50\n",
    "    ep_step = 5\n",
    "    df_res = ann_run(filename, prevday_max, prevday_min, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, futdays, s_flag = False)\n",
    "    df_res.to_csv(\"../HypTun/\"+pre+\"_Results_Epochs.csv\",index=False)\n",
    "    ep_min=10 \n",
    "    ep_max=10\n",
    "    ep_step = 1\n",
    "    \n",
    "\n",
    "    batch_min = 5 \n",
    "    batch_max= 15\n",
    "    df_res = ann_run(filename, prevday_max, prevday_min, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, futdays, s_flag = False)\n",
    "    df_res.to_csv(\"../HypTun/\"+pre+\"_Results_Batch.csv\",index=False)\n",
    "    batch_min= 10\n",
    "    batch_max=10\n",
    "\n",
    "\n",
    "    prevday_min = 0\n",
    "    prevday_max= 5\n",
    "    df_res = ann_run(filename, prevday_max, prevday_min, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, futdays, s_flag = False)\n",
    "    df_res.to_csv(\"../HypTun/\"+pre+\"_Results_Predays.csv\",index=False)\n",
    "    prevday_min = 1\n",
    "    prevday_max = 1\n",
    "\n",
    "\n",
    "    futdays = 14\n",
    "    df_res = ann_run(filename, prevday_max, prevday_min, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, futdays, s_flag = False)\n",
    "    df_res.to_csv(\"../HypTun/\"+pre+\"_Results_Futdays.csv\",index=False)\n",
    "    futdays=0\n",
    "\n",
    "#runHypTun(\"Baker\")\n",
    "runHypTun(\"AZ\")\n",
    "runHypTun(\"TX\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28c3a43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
